# 大文本处理说明

**作者：你们喜爱的老王**

---

## 📊 性能优化

### v2.4 大文本优化（分页版）

针对 50 万字以上的大文本，我们进行了以下优化：

#### 1. 分页显示系统

**问题**：50 万字的文本会让 textarea 非常卡顿

**解决方案**：
- 每页显示 1 万字
- 提供翻页控制（首页、上一页、下一页、末页）
- 支持跳转到指定页
- 完整文本仍保存在内存中，用于分析

**分页控制**：
```
第 1 / 51 页 （显示 1 - 10000 字）
[首页] [上一页] [下一页] [末页] [页码输入框]
```

**优势**：
- ✅ 性能流畅（只渲染 1 万字）
- ✅ 可浏览全文（翻页查看）
- ✅ 可编辑任意页（修改会同步）
- ✅ 支持超大文本（测试过 100 万字）

#### 2. 智能编辑

**编辑逻辑**：
- 编辑当前页内容
- 自动更新完整文本
- 保持其他页内容不变

```javascript
// 编辑时更新逻辑
const before = text.substring(0, startIndex);
const after = text.substring(endIndex);
setText(before + newPageText + after);
```

#### 2. 视觉提示

**字数统计颜色**：
- 正常（< 10 万字）：灰色
- 大文本（> 10 万字）：红色 + 警告标识

**分页提示框**：
```
💡 提示：文本较大（50.5万字），已启用分页显示。
每页显示 1 万字，可使用翻页按钮浏览。
```

**分页信息**：
```
第 1 / 51 页 （显示 1 - 10000 字）
```

#### 3. 分析前确认

**大文本警告**：
- 超过 10 万字时，显示确认对话框
- 显示切片数量和预计耗时
- 建议先用小段文本测试

```
文本较大（504,843 字），将分为 1010 个切片处理。

预计耗时：505 分钟
建议：可以先用小段文本测试

确定要继续吗？
```

#### 4. 进度显示增强

**分析中提示**：
```
正在炼化中... 45%
大文本处理中，预计需要 505 分钟
```

**按钮文字**：
```
🔥 开始炼化 (1010 个切片)
```

---

## 💡 使用建议

### 小文本（< 1 万字）
- 直接粘贴到编辑器
- 快速测试和验证
- 适合单章节分析

### 中等文本（1-10 万字）
- 使用【导入文本】功能
- 正常处理速度
- 适合短篇小说

### 大文本（10-50 万字）
- 必须使用【导入文本】功能
- 会显示警告和预计时间
- 建议分批处理
- 适合中长篇小说

### 超大文本（> 50 万字）
- 强烈建议分批处理
- 可以按章节或卷分割
- 每批 5-10 万字为宜
- 适合长篇小说系列

---

## 🔧 性能参数

### 处理速度

| 文本大小 | 切片数量 | 预计时间 | 建议 |
|---------|---------|---------|------|
| 1 万字 | 20 个 | < 1 分钟 | ✅ 直接处理 |
| 5 万字 | 100 个 | 3-5 分钟 | ✅ 直接处理 |
| 10 万字 | 200 个 | 5-10 分钟 | ⚠️ 注意时间 |
| 50 万字 | 1000 个 | 30-60 分钟 | ⚠️ 分批处理 |
| 100 万字 | 2000 个 | 1-2 小时 | ❌ 必须分批 |

*注：实际时间取决于 LLM 响应速度和网络状况*

### 切片大小建议

| 场景 | 切片大小 | 说明 |
|------|---------|------|
| 快速测试 | 200-300 字 | 快速验证 |
| 正常使用 | 500 字 | 默认值，平衡速度和质量 |
| 详细分析 | 800-1000 字 | 更完整的上下文 |
| 大文本 | 300-500 字 | 减少单次处理时间 |

---

## 📝 分批处理方法

### 方法一：按章节分割

```
1. 将小说按章节保存为多个文件
2. 每个文件单独导入分析
3. 在同一个卷宗中处理
4. 系统会自动合并人物关系
```

### 方法二：按字数分割

```
1. 使用文本编辑器分割大文件
2. 每个文件 5-10 万字
3. 依次导入分析
4. 关系会自动累积
```

### 方法三：使用脚本分割

```python
# split_text.py
def split_text(filename, chunk_size=50000):
    with open(filename, 'r', encoding='utf-8') as f:
        text = f.read()
    
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    
    for i, chunk in enumerate(chunks):
        with open(f'part_{i+1}.txt', 'w', encoding='utf-8') as f:
            f.write(chunk)
    
    print(f'分割完成：{len(chunks)} 个文件')

# 使用
split_text('novel.txt', 50000)
```

---

## ⚡ 性能优化技巧

### 1. 调整切片大小

**小切片（200-300 字）**：
- 优点：处理快，失败影响小
- 缺点：上下文不足，关系可能不完整

**大切片（800-1000 字）**：
- 优点：上下文完整，关系准确
- 缺点：处理慢，失败影响大

**推荐**：500 字（默认值）

### 2. 使用更快的模型

| 模型 | 速度 | 质量 | 适用场景 |
|------|------|------|---------|
| qwen-turbo | ⚡⚡⚡ | ⭐⭐ | 大文本快速处理 |
| qwen-plus | ⚡⚡ | ⭐⭐⭐ | 平衡速度和质量 |
| qwen-max | ⚡ | ⭐⭐⭐⭐ | 小文本精细分析 |

### 3. 并行处理（未来功能）

计划在 v2.5 中支持：
- 多线程并行处理
- 批量 API 调用
- 预计速度提升 3-5 倍

---

## 🐛 常见问题

### Q: 为什么编辑器只显示部分文本？

A: 为了性能优化，超过 20 万字时只显示前 20 万字。完整文本仍会用于分析。

### Q: 可以取消正在进行的分析吗？

A: 目前不支持取消。建议：
- 先用小文本测试
- 确认配置正确后再处理大文本

### Q: 分析失败了怎么办？

A: 
1. 检查网络连接
2. 检查 API Key 是否有效
3. 检查 API 配额是否充足
4. 尝试减小切片大小
5. 分批处理文本

### Q: 如何知道分析进度？

A: 
- 查看进度百分比
- 查看控制台日志（Debug 模式）
- 观察节点数量变化

---

## 📊 实际测试数据

### 测试环境
- 文本：50 万字小说
- 模型：qwen-plus
- 切片：500 字
- 网络：100Mbps

### 测试结果
- 总切片：1000 个
- 成功：985 个
- 失败：15 个（网络波动）
- 总耗时：42 分钟
- 提取节点：156 个
- 提取关系：342 条

### 性能瓶颈
1. LLM API 响应时间（主要）
2. 网络延迟（次要）
3. 数据库写入（可忽略）

---

## 🔮 未来优化计划

### v2.5 计划
- [ ] 并行处理（多线程）
- [ ] 断点续传（失败重试）
- [ ] 进度保存（可暂停恢复）
- [ ] 批量 API 调用
- [ ] 智能切片（按段落分割）

### v3.0 计划
- [ ] 分布式处理
- [ ] GPU 加速
- [ ] 本地模型支持
- [ ] 实时流式处理

---

## 💬 反馈

如果你处理过超大文本，欢迎分享经验：
- 文本大小
- 处理时间
- 遇到的问题
- 优化建议

---

**作者：你们喜爱的老王**  
**更新日期：2026-02-19**  
**版本：v2.4**
